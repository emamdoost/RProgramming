---
title: "Practical Machine Learning Course Project"
author: "Alireza Emam Doost"
date: "June 25, 2016"
output: 
  html_document: 
    fig_caption: yes
---


###Introduction  
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement wherein enthusiasts take measurements about themselves regularly to improve their health, to find patterns in their behavior. In this particular exercise the participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. Data has been gathered from accelerometers on the belt, forearm, arm, and dumbell of 6 participants and analyzed to fit a model that can now use this data to predict how the activity was performed.

The data for this project comes from this original source: <http://groupware.les.inf.puc-rio.br/har>.


##Initializing libraries and preparing Datasets
 

```r
#Load the necessary libraries
library(ggplot2)
library(lattice)
library(caret)
library(corrplot)
library(Rtsne)
library(xgboost)
library(stats)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
library(survival)
library(splines)
library(parallel)
library(gbm)
library(knitr)
knitr::opts_chunk$set(echo=TRUE, eval=TRUE, cache=TRUE)

#Getting and loading data
# URL of the training and testing data
Train_Dataset.url ="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
Test_Dataset.url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
# file names
Train_Dataset.name = "./data/pml-training.csv"
Test_Dataset.name = "./data/pml-testing.csv"
# if directory does not exist, create it
if (!file.exists("./data")) {
  dir.create("./data")
}
# if files does not exist, download the files
if (!file.exists(Train_Dataset.name)) {
  download.file(Train_Dataset.url, destfile=Train_Dataset.name)
}
if (!file.exists(Test_Dataset.name)) {
  download.file(Test_Dataset.url, destfile=Test_Dataset.name)
}
# load the CSV files as data.frame 
TrainingDS <- read.csv("./data/pml-training.csv", sep = ",", na.strings = c("", "NA"))
TestingDS <- read.csv("./data/pml-testing.csv", sep = ",", na.strings = c("", "NA"))
```
Let's have a look at the data and **classe** variable which we are going to predict:  

```r
str(TrainingDS, list.len=10)
```

```
## 'data.frame':	19622 obs. of  160 variables:
##  $ X                       : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ user_name               : Factor w/ 6 levels "adelmo","carlitos",..: 2 2 2 2 2 2 2 2 2 2 ...
##  $ raw_timestamp_part_1    : int  1323084231 1323084231 1323084231 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 ...
##  $ raw_timestamp_part_2    : int  788290 808298 820366 120339 196328 304277 368296 440390 484323 484434 ...
##  $ cvtd_timestamp          : Factor w/ 20 levels "02/12/2011 13:32",..: 9 9 9 9 9 9 9 9 9 9 ...
##  $ new_window              : Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 1 1 1 ...
##  $ num_window              : int  11 11 11 12 12 12 12 12 12 12 ...
##  $ roll_belt               : num  1.41 1.41 1.42 1.48 1.48 1.45 1.42 1.42 1.43 1.45 ...
##  $ pitch_belt              : num  8.07 8.07 8.07 8.05 8.07 8.06 8.09 8.13 8.16 8.17 ...
##  $ yaw_belt                : num  -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 ...
##   [list output truncated]
```

```r
table(TrainingDS$classe)
```

```
## 
##    A    B    C    D    E 
## 5580 3797 3422 3216 3607
```

```r
prop.table(table(TrainingDS$user_name, TrainingDS$classe), 1)
```

```
##           
##                    A         B         C         D         E
##   adelmo   0.2993320 0.1993834 0.1927030 0.1323227 0.1762590
##   carlitos 0.2679949 0.2217224 0.1584190 0.1561697 0.1956941
##   charles  0.2542421 0.2106900 0.1524321 0.1815611 0.2010747
##   eurico   0.2817590 0.1928339 0.1592834 0.1895765 0.1765472
##   jeremy   0.3459730 0.1437390 0.1916520 0.1534392 0.1651969
##   pedro    0.2452107 0.1934866 0.1911877 0.1796935 0.1904215
```

```r
prop.table(table(TrainingDS$classe))
```

```
## 
##         A         B         C         D         E 
## 0.2843747 0.1935073 0.1743961 0.1638977 0.1838243
```
###Cleaning Data
The first six columns contain an id, name and some timestamp data that may not be useful as predictors. Removing them from the training and testing datasets.There are near zero values in some columns and we are going to use the nearZeroVar method to identify those columns and exclude them from the model.

```r
dim(TrainingDS)
```

```
## [1] 19622   160
```

```r
# Removing first six columns
TrainingDS <- TrainingDS[, 7:160]
TestingDS  <- TestingDS[, 7:160]

AllNA    <- sapply(TrainingDS, function(x) mean(is.na(x))) > 0.95  
TrainingDS <- TrainingDS[, AllNA==FALSE]
TestingDS  <- TestingDS[, AllNA==FALSE]
dim(TrainingDS)
```

```
## [1] 19622    54
```
###Data Partitioning  
Partioning Training data set into two data sets, 60% for training, 40% for probing:

```r
inTrain <- createDataPartition(TrainingDS$classe, p=0.6, list=FALSE)
TrainingData <- TrainingDS[inTrain, ]
TrainingProb <- TrainingDS[-inTrain, ]
dim(TrainingData); dim(TrainingProb)
```

```
## [1] 11776    54
```

```
## [1] 7846   54
```
##Building Model
Three methods will be applied to model the regressions (in the Train dataset) and the best one (with higher accuracy when applied to the Test dataset) will be used for the quiz predictions. The methods are: Random Forests, Decision Tree and Generalized Boosted Model, as described below.
A Confusion Matrix is plotted at the end of each analysis to better visualize the accuracy of the models.

###1)Prediction with Decision Trees  

```r
set.seed(12345)
Model_1 <- rpart(classe ~ ., data=TrainingData, method="class")
fancyRpartPlot(Model_1, cex=0.1)
```

![plot of chunk unnamed-chunk-5](https://github.com/emamdoost/Practical-Machine-Learning-Course-Project/blob/master/PracticalMachineLearningCourseProject_files/figure-html/unnamed-chunk-5-1.png)

```r
Predictions_1 <- predict(Model_1, TrainingProb, type = "class")
CM_Model_1 <- confusionMatrix(Predictions_1, TrainingProb$classe)
CM_Model_1
```

```
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1991  351   62  124   48
##          B  143  909  137  119   75
##          C   15  108 1075  172   83
##          D   29   88   64  747  105
##          E   54   62   30  124 1131
## 
## Overall Statistics
##                                           
##                Accuracy : 0.746           
##                  95% CI : (0.7362, 0.7556)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.6767          
##  Mcnemar's Test P-Value : < 2.2e-16       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.8920   0.5988   0.7858  0.58087   0.7843
## Specificity            0.8958   0.9251   0.9416  0.95640   0.9578
## Pos Pred Value         0.7729   0.6573   0.7398  0.72314   0.8073
## Neg Pred Value         0.9543   0.9058   0.9542  0.92089   0.9517
## Prevalence             0.2845   0.1935   0.1744  0.16391   0.1838
## Detection Rate         0.2538   0.1159   0.1370  0.09521   0.1441
## Detection Prevalence   0.3283   0.1763   0.1852  0.13166   0.1786
## Balanced Accuracy      0.8939   0.7620   0.8637  0.76864   0.8711
```

```r
plot(CM_Model_1$table, col = CM_Model_1$byClass, 
     main = paste("Decision Tree Confusion Matrix: Accuracy =", 
                  round(CM_Model_1$overall['Accuracy'], 4)))
```

![plot of chunk unnamed-chunk-5](https://github.com/emamdoost/Practical-Machine-Learning-Course-Project/blob/master/PracticalMachineLearningCourseProject_files/figure-html/unnamed-chunk-5-2.png)

The resulting decision tree model has an accuracy of 73.7%. The confusion matrix shows the out of sample performance of the model.  

###2)Prediction with Random Forests  

```r
set.seed(12345)
Model_2 <- randomForest(classe ~ ., data=TrainingData)
Predictions_2 <- predict(Model_2, TrainingProb, type = "class")
CM_Model_2 <- confusionMatrix(Predictions_2, TrainingProb$classe)
CM_Model_2
```

```
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 2232    5    0    0    0
##          B    0 1509    5    0    0
##          C    0    4 1363    9    0
##          D    0    0    0 1276    3
##          E    0    0    0    1 1439
## 
## Overall Statistics
##                                          
##                Accuracy : 0.9966         
##                  95% CI : (0.995, 0.9977)
##     No Information Rate : 0.2845         
##     P-Value [Acc > NIR] : < 2.2e-16      
##                                          
##                   Kappa : 0.9956         
##  Mcnemar's Test P-Value : NA             
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            1.0000   0.9941   0.9963   0.9922   0.9979
## Specificity            0.9991   0.9992   0.9980   0.9995   0.9998
## Pos Pred Value         0.9978   0.9967   0.9906   0.9977   0.9993
## Neg Pred Value         1.0000   0.9986   0.9992   0.9985   0.9995
## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
## Detection Rate         0.2845   0.1923   0.1737   0.1626   0.1834
## Detection Prevalence   0.2851   0.1930   0.1754   0.1630   0.1835
## Balanced Accuracy      0.9996   0.9966   0.9972   0.9959   0.9989
```

```r
plot(CM_Model_2$table, col = CM_Model_2$byClass, 
     main = paste("Random Forest Confusion Matrix: Accuracy =", 
                  round(CM_Model_2$overall['Accuracy'], 4)))
```

![plot of chunk unnamed-chunk-6](https://github.com/emamdoost/Practical-Machine-Learning-Course-Project/blob/master/PracticalMachineLearningCourseProject_files/figure-html/unnamed-chunk-6-1.png)

The resulting Random Forests model has an accuracy of 99.7%. The confusion matrix shows the out of sample performance of the model.  

###3)Prediction with Generalized Boosted Regression  

```r
set.seed(12345)
Model_3  <- train(classe ~ ., data = TrainingData, method = "gbm",
                    trControl = trainControl(method = "repeatedcv", 
                                             number = 5, repeats = 1), verbose = FALSE)
```

```
## Loading required package: plyr
```

```r
Predictions_3 <- predict(Model_3, newdata=TrainingProb)
CM_Model_3 <- confusionMatrix(Predictions_3, TrainingProb$classe)
CM_Model_3
```

```
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 2226   24    0    0    0
##          B    6 1467    9    8    5
##          C    0   24 1353   19    2
##          D    0    3    6 1258   12
##          E    0    0    0    1 1423
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9848          
##                  95% CI : (0.9819, 0.9874)
##     No Information Rate : 0.2845          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.9808          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9973   0.9664   0.9890   0.9782   0.9868
## Specificity            0.9957   0.9956   0.9931   0.9968   0.9998
## Pos Pred Value         0.9893   0.9813   0.9678   0.9836   0.9993
## Neg Pred Value         0.9989   0.9920   0.9977   0.9957   0.9970
## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
## Detection Rate         0.2837   0.1870   0.1724   0.1603   0.1814
## Detection Prevalence   0.2868   0.1905   0.1782   0.1630   0.1815
## Balanced Accuracy      0.9965   0.9810   0.9910   0.9875   0.9933
```

```r
plot(CM_Model_3$table, col = CM_Model_3$byClass, 
     main = paste("GBM - Accuracy =", 
                  round(CM_Model_3$overall['Accuracy'], 4)))
```

![plot of chunk unnamed-chunk-7](https://github.com/emamdoost/Practical-Machine-Learning-Course-Project/blob/master/PracticalMachineLearningCourseProject_files/figure-html/unnamed-chunk-7-1.png)

The resulting Random Forests model has an accuracy of 98.8%. The confusion matrix shows the out of sample performance of the model.  

##Prediction  

The Random Forest model gave an accuracy of 99.7% on my TrainingProb dataset, which is much more better than the other models. The expected out-of-sample error is 100-99.7 = 0.3%, therefore the Random Forest model was selected to make the final predictions.


```r
Predictions_2 <- predict(Model_2, TestingDS, type = "class")
```

Here are the results of the prediction using the Random Forest model  

```
##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 
## Levels: A B C D E
```

Create the output format in files as required for submission  

```r
# Write the results to a text file for submission
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}
pml_write_files(Predictions_2)
```

##Conclusion  
In this assignment, the Random Forest model was used to predict the 20 test cases given as part of this exercise. The results were submitted for evaluation and declared completely. The accuracy obtained (accuracy = 99.7%, and out-of-sample error = 0.3%) is highly accure for this project besed on the used datasets.

